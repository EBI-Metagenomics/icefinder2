#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import time
import json

import logging
from pathlib import Path
from typing import Tuple, List

import shutil

from textwrap import dedent

from Bio import SeqIO
from ete3 import NCBITaxa
from Bio.SeqUtils import GC

from script import function
from script.helpers import run_command
from script.config import Config


logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)


def rename_fasta(runID: str, fasta: str, config: Config):
    """Rename the contigs in the fasta file.
    It returns a dictionary with the mapping between the "new" contig names and the original ones
    Also, it returns the path to the renamed file
    """

    contig_ids_mapping = {}

    renamed_fasta = config.run_id_dir / "runID_renamed.fasta"

    sequences = []

    for idx, seq_record in enumerate(SeqIO.parse(fasta, "fasta"), start=1):
        if len(seq_record.id) > 15:
            realID = seq_record.id[:15] + "..."
        else:
            realID = seq_record.id
        contigID = f"contig_{idx}"

        seq_record.id = contigID
        sequences.append(seq_record)

        contig_ids_mapping[contigID] = realID

    with open(renamed_fasta, "w") as fasta_writer:
        SeqIO.write(sequences, fasta_writer, "fasta")

    return contig_ids_mapping, renamed_fasta


def get_time():
    return time.asctime(time.localtime(time.time()))


def get_kraken_taxonomy(runID, fasta_file: str, config: Config):
    """Run Kraken2 on the FASTA file and return relevant outputs.

    This function executes Kraken2 using the provided configuration and
    returns a dictionary of species taxonomies along with output files.

    :param runID: A unique identifier for the run.
    :type runID: str

    :param fasta_file: Path to the input FASTA file containing sequences.
    :type fasta_file: str

    :param config: Configuration
    :type config: Config

    :return: A tuple containing:
        - drawout (Path): Path to the HTML output generated by Kraken2.
        - spdict (Dict[str, Union[str, Any]]): Dictionary mapping sequence IDs
            to their species taxonomies or '-' if unclassified.
        - report (Path): Path to the Kraken2 report file.
    :rtype: tuple[Path, Dict[str, Union[str, Any]], Path]
    """
    report = config.run_id_dir / (runID + "_kraken.report")
    output = config.run_id_dir / (runID, runID + "_kraken.output")
    drawout = config.run_id_dir / (runID, "kraken.html")

    kraken2_command = [
        config.kraken,
        "--db", config.kraken,
        "--report", report,
        "--output", output,
        fasta_file,
    ]

    run_command(kraken2_command)

    spdict = {}
    with open(output, "r") as taxainfo:
        for line in taxainfo.readlines():
            lines = line.strip().split("\t")
            ID, taxid = lines[1], lines[2]
            if taxid == "0":
                spdict[ID] = "-"
            else:
                spname, strainame = get_ranks(taxid)
                spdict[ID] = spname

    return drawout, spdict, report


def get_ranks(taxid: int, config: Config):
    ncbi = NCBITaxa(dbfile=config.ete3ncbi_database)
    lineage = ncbi.get_lineage(taxid)
    names = ncbi.get_taxid_translator(lineage)
    lineage2ranks = ncbi.get_rank(names)
    ranks2lineage = dict((rank, taxid) for (taxid, rank) in lineage2ranks.items())

    strainid = ""
    if "species" in ranks2lineage:
        spid = ranks2lineage["species"]
        if "strain" in ranks2lineage:
            strainid = ranks2lineage["strain"]
        else:
            strainid = ""
    elif "genus" in ranks2lineage:
        spid = ranks2lineage["genus"]
        strainid = ""
    elif "phylum" in ranks2lineage:
        spid = ranks2lineage["phylum"]
        strainid = ""

    try:
        spname = list(ncbi.get_taxid_translator([spid]).values())[0]
    except:
        spname = "-"

    strainame = ""
    if strainid:
        strainame = list(ncbi.get_taxid_translator([strainid]).values())[0]

    return spname, strainame


def get_fasta_stats(runID: str, renamed_fasta_file: Path, config: Config):
    """Get the total lenght of the fasta, number of contigs and N50"""
    seqkit_stats_command = [config.seqkit, "stats", "-a", renamed_fasta_file]

    cmd_result = run_command(seqkit_stats_command)

    for line in cmd_result.strip().split('\n'):
        lines = line.strip().split()
        if lines[0] != "file":
            length = lines[4]
            count = lines[3]
            n50 = lines[12]

    stats_file = config.run_id_dir / (runID + "_info.json")

    base_dict = {
        "JobID": runID,
        "Submission date": get_time(),
        "Total length": length + " bp",
        "Contig number": count,
        "Sequence N50": n50 + " bp",
    }
    with open(stats_file, "w") as gein2:
        json.dump(base_dict, gein2, indent=4)

    return stats_file


def gc(fasta_file, start, end):
    record = SeqIO.read(fasta_file, "fasta")
    sequence = record.seq[start - 1 : end]
    gcs = str("%.2f" % GC(sequence))

    return gcs


def calculate_gc(fasta_file, start, end, window_size, step_size):
    record = SeqIO.read(fasta_file, "fasta")
    if start == 0:
        start = 1
    sequence = record.seq[start - 1 : end]

    windows = []
    gc_contents = []
    pos = []
    j = start / 1000 + 0.025
    for i in range(0, len(sequence) - window_size + 1, step_size):
        window = sequence[i : i + window_size]
        gc_content = GC(window)
        gc_contents.append(gc_content)
        pos.append(round(j, 4))
        j += 0.05

    gcdict = {
        "xData": pos,
        "datasets": [
            {
                "name": "",
                "data": gc_contents,
                "unit": "%",
                "type": "line",
                "valueDecimals": 1,
            }
        ],
    }

    return gcdict


def run_prodigal(runID: str, renamed_fasta: Path, config: Config) -> Tuple[Path, Path]:
    """Run Prodigal to annotate genes from a FASTA file.

    This function executes Prodigal with the specified parameters to generate gene annotations
    in GFF format. It creates two output files:
    - A FASTA file of annotated sequences (annotation_fa)
    - A GFF file of annotation features (annotation_gff)

    The Prodigal command is constructed and executed using `run_command()`.

    :param runID: A string identifier for the current run.
    :type runID: str

    :param renamed_fasta: The input FASTA file path containing sequences to be annotated.
    :type renamed_fasta: Path

    :param config: Configuration object containing necessary settings and paths.
    :type config: Config

    :return: A tuple containing the paths to the annotated FASTA file and the GFF file.
    :rtype: Tuple[Path, Path]
    """
    annotation_faa = config.run_id_dir / f"{runID}.faa"
    annotation_gff = config.run_id_dir / f"{runID}.gff"

    prodigal_annotation_command = [
        config.prodigal,  # Assuming prodigal is stored in config
        "-c",
        "-m",
        "-q",
        "-p",
        "meta",
        "-f",
        "gff",
        "-i",
        renamed_fasta,
        "-a",
        annotation_faa,
        "-o",
        annotation_gff,
    ]

    run_command(prodigal_annotation_command)

    return annotation_faa, annotation_gff


def scanf(hmmlist):
    ICEcount = []
    for line in hmmlist:
        if "MOB" in line:
            ICEcount.append("MOB")
        elif "t4cp" in line or "tcpA" in line:
            ICEcount.append("t4cp")
        elif "FA" in line:
            ICEcount.append("T4SS")
        elif line in [
            "Phage_integrase",
            "UPF0236",
            "Recombinase",
            "rve",
            "TIGR02224",
            "TIGR02249",
            "TIGR02225",
            "PB001819",
        ]:
            ICEcount.append("Int")
        else:
            ICEcount.append("T4SS")
    if (
        ICEcount.count("MOB")
        and ICEcount.count("t4cp")
        and ICEcount.count("Int")
        and ICEcount.count("T4SS") >= 5
    ):
        return True
    else:
        return False


def prescan(runID: str, renamed_fasta_file: Path, config: Config) -> List[str]:
    """Perform a prescan analysis using HMMER's hmmscan on annotated sequences.

    This function first runs Prodigal to annotate genes from a given FASTA file,
    then uses HMMER's hmmscan to identify potential ICE (Integrative and Conjugative Element)
    sequences based on the annotations. The results are filtered based on a significance threshold.

    :param runID: A string identifier for the current run.
    :type runID: str

    :param renamed_fasta_file: The input FASTA file path containing sequences to be annotated.
    :type renamed_fasta_file: Path

    :param config: Configuration object containing necessary settings and paths.
    :type config: Config

    :return: A list of keys representing identified ICE sequences that passed the filtering criteria.
    :rtype: List[str]

    This function generates a table output file from the hmmscan command, which is then parsed
    to extract relevant information. Only sequences with an E-value below 0.00001 are considered,
    and duplicates are removed based on the first two parts of the identifier.
    """
    annotation_faa, annotation_gff = run_prodigal(runID, renamed_fasta_file, config)

    hmmscan_tblout = config.run_id_dir / f"{runID}_prescan"

    hmm2scan_command = [
        config.hmmscan,
        "--tblout",
        hmmscan_tblout,
        config.databases_dir / "ICEscan.hmm",
        annotation_faa,
    ]

    run_command(hmm2scan_command)

    icedict = {}
    chosen = []
    with open(hmmscan_tblout, "r") as outfile:
        for line in outfile.readlines():
            if not line.startswith("#"):
                lines = line.strip().split()
                if lines[2] in icedict:
                    continue
                id_parts = lines[2].split("_")
                key = "_".join(id_parts[0:2])
                if float(lines[4]) < 0.00001:
                    if key in icedict:
                        icedict[key].append(lines[0])
                    else:
                        icedict[key] = [lines[0]]
    for k, v in icedict.items():
        if scanf(v):
            chosen.append(k)

    return chosen


def run_prokka(runID: str, fasta_file: str, config: Config) -> Tuple[Path, Path]:
    """Run Prokka to annotate a given FASTA file.

    This function executes Prokka to perform gene annotation on the specified FASTA file.
    It generates output files in GenBank format and protein FASTA format. The function checks
    for the existence of the expected output files and raises an error if they are not found.

    :param runID: A string identifier for the current run, used as a prefix for output files.
    :type runID: str

    :param fasta_file: The input FASTA file path containing sequences to be annotated.
    :type fasta_file: str

    :param config: Configuration object containing necessary settings and paths.
    :type config: Config

    :return: A tuple containing the paths to the generated protein FASTA file, GenBank file and a FFN file
    :rtype: Tuple[Path, Path, Path, Path]

    The Prokka command is constructed with the specified parameters, including options for
    fast processing, quiet mode, and the number of CPU cores to use. The output files are
    stored in the directory specified by the configuration object.
    """
    prokka_command = [
        config.prokka,
        fasta_file,
        "--force",
        "--fast",
        "--quiet",
        "--cdsrnaolap",
        "--cpus", config.prokka_cores,
        "--outdir", config.outdir,
        "--prefix", runID
    ]
    
    run_command(prokka_command)

    # Check expected faa and gff result
    prokka_faa = config.outdir / (runID + ".faa")
    prokka_ffn = config.outdir / (runID + ".ffn")
    prokka_gff = config.outdir / (runID + ".gff")
    prokka_gbk = config.outdir / (runID + ".gbk")

    if not prokka_faa.exists():
        raise ValueError(f"Prokka output {prokka_faa} doesn't exist")

    if not prokka_ffn.exists():
        raise ValueError(f"Prokka output {prokka_ffn} doesn't exist")

    if not prokka_gff.exists():
        raise ValueError(f"Prokka output {prokka_gff} doesn't exist")

    if not prokka_gbk.exists():
        raise ValueError(f"Prokka output {prokka_gbk} doesn't exist")
    
    return prokka_faa, prokka_gbk, prokka_gff, prokka_ffn


def run_icescan(runID: str, annotation_faa: Path, config: Config) -> Path:
    """Run ICEscan to identify integrative and conjugative elements from annotated sequences.

    This function executes ICEscan using the specified parameters to analyze the provided
    annotated FASTA file. It creates a directory for the results and checks for the existence
    of the expected output file.

    :param runID: A string identifier for the current run, used to name the output directory.
    :type runID: str

    :param annotation_faa: The input protein FASTA file path containing annotated sequences.
    :type annotation_faa: Path

    :param config: Configuration object containing necessary settings and paths.
    :type config: Config

    :return: The path to the output TSV file containing the results of the ICEscan analysis.
    :rtype: Path

    The function constructs the ICEscan command with specified options, including the database type,
    model directory, and coverage profile. It checks for the existence of the output file
    `all_systems.tsv` and raises an error if it is not found.
    """
    ice_results = config.run_id_dir / (runID + "_ICE")
    if ice_results.exists():
        ice_results.rmdir()

    ice_results.mkdir(exist_ok=False)

    cmd = [
        config.macsyfinder,
        "--db-type", "ordered_replicon",
        "--hmmer", config.hmmsearch,
        "--models-dir", config.defensefinder_database,
        "--models", "ICEscan", "all",
        "--replicon-topology", "linear",
        "--coverage-profile", "0.3",
        "--sequence-db", annotation_faa,
        "-o", ice_results
    ]

    run_command(cmd)

    #Â example: /tmp/tmpq4dd62bb/SRS146999/SRS146999_contig_8_ICE/all_systems.tsv
    all_systems_tsv = ice_results / "all_systems.tsv"

    if not all_systems_tsv.exists():
        raise ValueError(f"ICEScan output missing {all_systems_tsv}")

    return all_systems_tsv


def get_gff(runID: str, annotation_gff: Path, config: Config):

    with open(annotation_gff, "r") as gffin:
        trnadict = {}
        posdict = {}
        for line in gffin.readlines():
            if "ID=" in line:
                lines = line.strip().split("\t")
                ids = lines[8].split(";")[0].split("=")[1]
                header = ids.split("_")[0]
                product = lines[8].split("product=")[1]
                pos = [lines[3], lines[4], lines[6], product]
                if lines[2] == "tRNA" or lines[2] == "tmRNA":
                    trnadict[ids] = product
                posdict[ids] = pos
                totalnum = get_num(ids)

    return trnadict, posdict, header, totalnum


def get_num(ID):
    return int(ID.split("_")[1].lstrip("0"))


def find_max_distance(numbers):
    max_distance = -1
    max_distance_index = -1

    for i in range(len(numbers) - 1):
        distance = abs(numbers[i] - numbers[i + 1])
        if distance > max_distance:
            max_distance = distance
            max_distance_index = i

    if max_distance_index == -1:
        return None

    return numbers[max_distance_index], numbers[max_distance_index + 1]


def pos_tag(pos, posdict, ICE, final, totalnum, dirtag):
    tICE = ICE
    tfinal = final
    for k, v in posdict.items():
        vstart, vend = int(v[0]), int(v[1])
        if int(pos) <= vend:
            if dirtag == "s":
                tICE = get_num(k)
                tfinal = max(1, tICE - 5)
            else:
                if vstart > int(pos):
                    tICE = get_num(k) - 1
                else:
                    tICE = get_num(k)
                tfinal = min(totalnum, tICE + 5)
            break
    return tICE, tfinal


def merge_tRNA(runID, ICEdict, DRlist, annotation_gff, config):
    trnadict, posdict, header, totalnum = get_gff(runID, annotation_gff, config)
    fICE = get_num(next(iter(ICEdict)))
    eICE = get_num(list(ICEdict.keys())[-1])

    nfICEnum = max(1, fICE - 5)
    neICEnum = min(totalnum, eICE + 5)

    ICEtagnum = [nfICEnum, neICEnum]
    trnalist = []
    for key, value in trnadict.items():
        if nfICEnum <= get_num(key) <= neICEnum:
            ICEtagnum.append(get_num(key))
            trnalist.append(value)

    ICEtagnum.sort()
    finalstart, finalend = find_max_distance(ICEtagnum)

    myDR1 = posdict[zill(header, fICE)][0]
    myDR2 = ""
    myDR3 = ""
    myDR4 = posdict[zill(header, eICE)][1]

    if trnalist:
        if finalstart == nfICEnum:
            eICE = finalend
            finalend = min(totalnum, finalend + 5)
            myDR4 = posdict[zill(header, eICE)][1]
            for line in DRlist:
                DRs = line.split("|")
                if int(DRs[3]) - int(DRs[0]) > 500000:
                    continue
                if int(DRs[3]) - int(DRs[0]) < 5000:
                    continue
                if (
                    int(posdict[zill(header, eICE)][0])
                    < int(DRs[3])
                    < int(posdict[zill(header, eICE)][1])
                ):
                    checktrna = 0
                    for key, value in trnadict.items():
                        if int(DRs[0]) <= value[0] <= int(DRs[3]) and int(
                            DRs[0]
                        ) <= value[1] <= int(DRs[3]):
                            checktrna += 1
                    if checktrna >= 2:
                        break

                    fICE, finalstart = pos_tag(
                        DRs[0], posdict, fICE, finalstart, totalnum, "s"
                    )
                    myDR1 = DRs[0]
                    myDR2 = DRs[1]
                    myDR3 = DRs[2]
                    myDR4 = DRs[3]
                    break

        elif finalend == neICEnum:
            fICE = finalstart
            finalstart = max(1, finalstart - 5)
            myDR1 = posdict[zill(header, fICE)][0]
            for line in DRlist:
                DRs = line.split("|")
                if int(DRs[3]) - int(DRs[0]) > 500000:
                    continue
                if int(DRs[3]) - int(DRs[0]) < 5000:
                    continue
                if (
                    int(posdict[zill(header, fICE)][0])
                    < int(DRs[0])
                    < int(posdict[zill(header, fICE)][1])
                ):
                    checktrna = 0
                    for key, value in trnadict.items():
                        if int(DRs[0]) <= value[0] <= int(DRs[3]) and int(
                            DRs[0]
                        ) <= value[1] <= int(DRs[3]):
                            checktrna += 1
                    if checktrna >= 2:
                        break
                    eICE, finalend = pos_tag(
                        DRs[3], posdict, eICE, finalend, totalnum, "e"
                    )
                    myDR1 = DRs[0]
                    myDR2 = DRs[1]
                    myDR3 = DRs[2]
                    myDR4 = DRs[3]
                    break

    return (
        myDR1,
        myDR2,
        myDR3,
        myDR4,
        fICE,
        eICE,
        finalstart,
        finalend,
        posdict,
        header,
        trnalist,
    )


def get_DR(runID, annotation_fa: Path, config: Config):
    DRindex = config.run_id_dir / (runID + "_DR")
    DRout = config.run_id_dir / (runID + "_DRout")

    maktree_cmd = [
        config.mkvtree,
        "-db", annotation_fa,
        "-indexname", DRindex,
        "-dna",
        "-pl",
        "-lcp",
        "-suf",
        "-tis",
        "-ois",
        "-bwt",
        "-bck",
        "-sti1",
    ]

    run_command(maktree_cmd)

    vmatch_cmd = [config.vmatch, "-l", "15", DRindex]

    run_command(vmatch_cmd, output_file=DRout)

    DRlist = []
    with open(DRout, "r") as DRin:
        for line in DRin.readlines():
            lines = line.strip().split()
            if not line.startswith("#"):
                DR = [
                    lines[2],
                    str(int(lines[2]) + int(lines[0])),
                    lines[6],
                    str(int(lines[6]) + int(lines[4])),
                ]
                DRlist.append("|".join(DR))
    return DRlist


def get_ICE(runID: str, annotations_faa: Path, annotations_fa: Path, annotation_gff: Path, config: Config):

    all_systems_tsv = run_icescan(runID, annotations_faa, config)

    ICEdict = {}
    infodict = {}

    with open(all_systems_tsv, 'r') as file:
        for line in file:
            if line.startswith('#'):
                continue
            if "Chromosome" not in line:
                continue

            row_data = line.strip().split("\t")

            # TODO: I'm not sure about this
            if "ICEscan/Chromosome/IME" in row_data[4]:
            # if "UserReplicon_IME" in row_data[5]:
                continue

            gbname = row_data[1]
            tags = get_feat(row_data[2])
            mpf = row_data[4].split('/')[-1].split('_')[1]

            if 'Relaxase@' in tags:
                mob = tags.split('@')[1]
            else:
                mob = ''

            ICEtag = 'ICE' + row_data[5].split('_')[-1]

            ICEdict.setdefault(ICEtag, {})[gbname] = tags

            if ICEtag not in infodict:
                infodict[ICEtag] = {'mob': [], 'mpf': []}
            if mob and mob not in infodict[ICEtag]['mob']:
                infodict[ICEtag]['mob'].append(mob)
            if mpf and mpf not in infodict[ICEtag]['mpf']:
                infodict[ICEtag]['mpf'].append(mpf)

    dictICE = {}
    posdict = {}
    trnalist = []
    header = ""

    DRlist = get_DR(runID, annotations_fa, config)

    for key, value in ICEdict.items():
        (
            myDR1,
            myDR2,
            myDR3,
            myDR4,
            fICE,
            eICE,
            finalstart,
            finalend,
            posdict,
            header,
            trnalist,
        ) = merge_tRNA(runID, value, DRlist, annotation_gff, config)

        dictICE[key] = [myDR1, myDR2, myDR3, myDR4, fICE, eICE, finalstart, finalend]

    return dictICE, ICEdict, posdict, header, trnalist, infodict


def zill(header, num):
    return header + "_" + str(num).zfill(5)


def get_args(
    argdict, vfdict, isdict, dfdict, metaldict, popdict, symdict, gene, feature, product
):
    feature = [feature]
    product = [product]

    if gene in argdict:
        feature.append("AR")
        product.append(argdict[gene])
    if gene in vfdict:
        feature.append("VF")
        product.append(vfdict[gene])
    if gene in isdict:
        feature.append("IS")
        product.append(isdict[gene])
    if gene in dfdict:
        feature.append("Defense")
        product.append(dfdict[gene])

    if gene in metaldict:
        feature.append("Metal")
        product.append(metaldict[gene])
    if gene in popdict:
        feature.append("Degradation")
        product.append(popdict[gene])
    if gene in symdict:
        feature.append("Symbiosis")
        product.append(symdict[gene])

    feature = "; ".join(list(filter(None, feature)))
    product = "; ".join(list(filter(None, product)))

    return feature, product


def oritseq(runID, regi, infile, start: int, end: int, config: Config):
    oritseq = "-"
    
    fafile = config.run_id_dir / (regi + "_fororit.fa")

    with open(fafile, "w") as orif:
        seq = getfa(infile, start, end)
        orif.write(">fororit\n")
        orif.write(seq)

    blastn_out = config.run_id_dir / (regi + "_oriTout")

    blast_cmd = [
        config.blastn,
        "-db", config.orit_blast_database,
        "-query", fafile,
        "-evalue", "0.01",
        "-word_size", "11",
        "-outfmt", "6 std qlen slen",
        "-num_alignments", "1",
        "-out", blastn_out,
    ]

    run_command(blast_cmd)

    with open(blastn_out, "r") as oritout:
        for line in oritout.readlines():
            lines = line.strip().split()
            if lines[0]:
                matchl = int(lines[3])
                slen = int(lines[13])
                ident = float(lines[2])
                hvalue = (matchl / slen) * ident
                if hvalue > 0.49:
                    oritseq = getfa(fafile, str(int(lines[6]) - 1), lines[7])
                    break
    return oritseq


def get_feat(feat):
    featuredict = {
        "Phage_integrase": "Integrase",
        "UPF0236": "Integrase",
        "Recombinase": "Integrase",
        "rve": "Integrase",
        "TIGR02224": "Integrase",
        "TIGR02249": "Integrase",
        "TIGR02225": "Integrase",
        "PB001819": "Integrase",
    }
    if feat in featuredict:
        return "Integrase@" + feat
    elif "T4SS_MOB" in feat:
        tag = feat.split("_")[1]
        return "Relaxase@" + tag
    elif "t4cp" in feat:
        tag = feat.split("_")[1]
        return "T4CP@" + tag
    elif "tcpA" in feat:
        tag = feat.split("_")[1]
        return "T4CP@" + tag
    else:
        return "T4SS@" + feat.replace("T4SS_", "")


def getcolor(feature, product):
    coldict = {
        "DR": "black",
        "Gene": "#C0C0C0",
        "Hyp": "#DCDCDC",
        "Integrase": "blue",
        "Transposase": "yellow",
        "T4SS": "lightpink",
        "T4CP": "orange",
        "Relaxase": "brown",
        "AR": "red",
        "tRNA": "black",
        "Flank": "gray",
        "VF": "#ba8448",
        "Defense": "#00B050",
        "Metal": "#03A89E",
        "Degradation": "#640B0F",
        "Symbiosis": "#FFFFCD",
    }

    namedict = {
        "Hyp": "Hypothetical protein",
        "Gene": "Other gene",
        "AR": "Antibiotic resistance gene",
        "VF": "Virulence factor",
        "Metal": "Metal resistance",
        "Flank": "Flank region",
        "Defense": "Defense system",
        "Transposase": "Transposase",
        "Relaxase": "Relaxase",
        "T4CP": "T4CP",
        "T4SS": "T4SS",
        "Integrase": "Integrase",
        "Degradation": "Degradation",
        "Symbiosis": "Symbiosis",
    }

    if "Integrase" in feature:
        feature = "Integrase"
    elif "T4SS" in feature:
        feature = "T4SS"
    elif "T4CP" in feature:
        feature = "T4CP"
    elif "Relaxase" in feature:
        feature = "Relaxase"
    elif "IS" in feature:
        feature = "Transposase"
    elif "VF" in feature:
        feature = "VF"
    elif "AR" in feature:
        feature = "AR"
    elif "Defense" in feature:
        feature = "Defense"
    elif "Metal" in feature:
        feature = "Metal"
    elif "Degradation" in feature:
        feature = "Degradation"
    elif "Symbiosis" in feature:
        feature = "Symbiosis"

    elif feature == "Flank":
        feature == "Flank"
    elif feature == "":
        if product == "hypothetical protein":
            feature = "Hyp"
        else:
            feature = "Gene"
    else:
        feature = "Gene"

    return coldict[feature], namedict[feature]


def gstrand(instra):
    strands = {"+": 1, "-": -1}
    return strands[instra]


def getfa(infile, s, e):
    seq_record = SeqIO.read(infile, "fasta")
    sequence = seq_record.seq[int(s) : int(e)]
    return str(sequence)


def get_map(
    sprunID: str,
    fasta_file: Path,
    # sequence_taxonomy_mapping: dict,
    contig_ids_mapping: dict,
    annotation_faa: Path,
    annotation_fa: Path,
    annotation_gff: Path,
    annotation_ffn: Path,
    config: Config
):
    results_js_folder = config.outdir / "js"
    results_js_folder.mkdir(parents=True, exist_ok=True)
   

    dictICE, ICEdict, posdict, header, trnalist, infodict = get_ICE(sprunID, annotation_faa, annotation_fa, annotation_gff, config)

    argdict, vfdict, isdict, dfdict, metaldict, popdict, symdict = function.get_blast_results(sprunID, annotation_faa, annotation_ffn, config)

    ICEss = {}
    for key, value in dictICE.items():
        genelist = []
        region = sprunID + "_" + key
        region_js_prefix = "contig_" + sprunID.split("_contig_", 1)[-1] + "_" + key
    
        genefile = config.outdir / f"{region}_gene.json"
        infofile = config.outdir / f"{region}_info.json"

        gcjs = results_js_folder / f"{region_js_prefix}_gc.js"
        mapfile = results_js_folder / f"{region_js_prefix}.js"
        htmlfile = config.outdir / f"{region}.html"
    
        [myDR1, myDR2, myDR3, myDR4, fICE, eICE, finalstart, finalend] = value

        start = finalstart
        while start < fICE:
            gene = zill(header, start)
            s, e, strand, pro = posdict[gene]
            pos = s + ".." + e + " [" + strand + "], " + str(int(e) - int(s) + 1)

            feature = "Flank"
            product = pro
            feature, product = get_args(
                argdict,
                vfdict,
                isdict,
                dfdict,
                metaldict,
                popdict,
                symdict,
                gene,
                feature,
                product,
            )
            if "hypothetical protein;" in product:
                product = product.replace("hypothetical protein;", "")

            start += 1
            content = {"gene": gene, "pos": pos, "prod": product, "featu": feature}
            genelist.append(content)

        mov = fICE
        while mov <= eICE:
            gene = zill(header, mov)
            s, e, strand, pro = posdict[gene]
            pos = s + ".." + e + " [" + strand + "], " + str(int(e) - int(s) + 1)

            if gene in ICEdict[key]:
                [feature, pro11] = ICEdict[key][gene].split("@")
            else:
                feature, pro11 = "", ""

            if pro11:
                if pro == "hypothetical protein":
                    product = pro11
                else:
                    product = pro + ", " + pro11
            else:
                product = pro

            feature, product = get_args(
                argdict,
                vfdict,
                isdict,
                dfdict,
                metaldict,
                popdict,
                symdict,
                gene,
                feature,
                product,
            )
            mov += 1
            content = {"gene": gene, "pos": pos, "prod": product, "featu": feature}
            genelist.append(content)

        while mov <= finalend:
            gene = zill(header, mov)
            s, e, strand, pro = posdict[gene]
            pos = s + ".." + e + " [" + strand + "], " + str(int(e) - int(s) + 1)

            feature = "Flank"
            product = pro
            feature, product = get_args(
                argdict,
                vfdict,
                isdict,
                dfdict,
                metaldict,
                popdict,
                symdict,
                gene,
                feature,
                product,
            )
            if "hypothetical protein;" in product:
                product = product.replace("hypothetical protein;", "")

            mov += 1
            content = {"gene": gene, "pos": pos, "prod": product, "featu": feature}
            genelist.append(content)

        with open(genefile, "w") as gene_file:
            json.dump(genelist, gene_file, indent=4)

        contigID = sprunID.split("_", 1)[1]

        sgene = zill(header, fICE)
        egene = zill(header, eICE)
        s1, e1, strand1, pro1 = posdict[sgene]
        s2, e2, strand2, pro2 = posdict[egene]
        if myDR1 == "0":
            myDR1 = "1"

        ICEss[region] = "|".join([myDR1, myDR4, str(fICE), str(eICE)])

        # host = sequence_taxonomy_mapping[contigID] TODO: disabled kraken
    
        gcc = gc(fasta_file, int(myDR1), int(myDR4))
        source = contig_ids_mapping[contigID]

        if myDR2:
            DR1 = getfa(fasta_file, myDR1, myDR2)
            DR2 = getfa(fasta_file, myDR3, myDR4)
            DRw = f"""\
                attL: {myDR1}..{myDR2} ({DR1})  
                attR: {myDR3}..{myDR4} ({DR2})
            """.strip()

        else:
            DRw = "-"

        oritseqs = oritseq(sprunID, region, fasta_file, myDR1, myDR4, config)

        ICEinfo = {
            "Contig source": source,
            # "Host Strain": host, TODO: disabled kraken
            "GC Content (%)": gcc,
            "Length (bp)": str(int(e2) - int(s1) + 1),
            "oriT seq": oritseqs,
            "DRs": DRw,
            "Relaxase Type": ",".join(infodict[key]["mob"]),
            "Mating pair formation systems": ",".join(infodict[key]["mpf"]),
            "Close to tRNA": ",".join(trnalist),
        }
        with open(infofile, "w") as info_file:
            json.dump(ICEinfo, info_file, indent=4)

        i = 1
        mapzlist = []
        mapflist = []
        for gene in genelist:
            color, name = getcolor(gene["featu"], gene["prod"])
            start = gene["pos"].split(" ")[0].split("..")[0]
            end = gene["pos"].split(" ")[0].split("..")[1]
            strand = gstrand(gene["pos"].split("[")[1].split("]")[0])
            product = gene["prod"]

            if product == "":
                product = "hypothetical protein"

            anno = {
                "start": start,
                "end": end,
                "strand": strand,
                "locus_tag": f"M{i}",
                "type": "others",
                "color": color,
                "description": dedent(f"""\
                    Location: {gene["pos"].split(" ")[0]} (
                    {gene["pos"].split(" ")[2]} bp)<br>
                    Type: {name}<br>
                    Detail: {product}
                """).strip(),
            }

            if strand == 1:
                mapzlist.append(anno)
            else:
                mapflist.append(anno)
            i += 1

        head = "var borders = [];\nvar tta_codons = [];\nvar orfs ="
        start_position = genelist[0]["pos"].split(" ")[0].split("..")[0]
        end_position = genelist[-1]["pos"].split(" ")[0].split("..")[1]

        gcdict = calculate_gc(fasta_file, int(start_position), int(end_position), 500, 50)

        with open(config.icefinder2_static_directory / "gcmap.js", "r") as original_file:
            original_content = original_file.read()

        with open(gcjs, "w") as gein2:
            gein2t = "var jsonData = " + str(gcdict) + ";"
            gein2.write(gein2t)
            gein2.write(original_content)

        maps = dedent(f"""\
            {str(mapzlist)};
            var orfs2 = {str(mapflist)};
            var clusterf2 = {{
                start: {start_position},
                end: {end_position},
                idx: 1,
                orfs: orfs,
                borders: borders,
                tta_codons: tta_codons,
                label: '',
                unordered: true
            }};
            var clusterr2 = {{
                start: {start_position},
                end: {end_position},
                idx: 2,
                orfs: orfs2,
                borders: borders,
                tta_codons: tta_codons,
                label: '',
                unordered: true
            }};
            svgene.drawClusters("{region_js_prefix}", [clusterf2, clusterr2], 50, 920);
        """)

        with open(mapfile, "w") as map_file:
            map_file.write(head + maps)

        with open(htmlfile, "w") as htmlfile_file:
                with open(config.icefinder2_static_directory / "view.html", "r") as viewhtml_filehandler:
                    htmlfile_file.write(viewhtml_filehandler.read().replace("XXXX", region_js_prefix))

    return ICEss


def copy_files(source_dir, destination_dir):
    if os.path.isfile(source_dir):
        shutil.copy(source_dir, destination_dir)
    elif os.path.isdir(source_dir):
        if not os.path.exists(destination_dir):
            os.makedirs(destination_dir)
        for item in os.listdir(source_dir):
            source_item = os.path.join(source_dir, item)
            destination_item = os.path.join(destination_dir, item)
            if os.path.isdir(source_item):
                copy_files(source_item, destination_item)
            else:
                shutil.copy2(source_item, destination_item)


def delete_folders_starting_with_keyword(dir, keyword):
    for dirpath, dirnames, filenames in os.walk(dir, topdown=False):
        for dirname in dirnames:
            if dirname.startswith(keyword):
                folder_to_remove = os.path.join(dirpath, dirname)
                shutil.rmtree(folder_to_remove)


def get_fasta(
    runID: str,
    contig_fasta: Path,
    id_dict: dict,
    key: str,
    start: int,
    end: int,
    stag: str,
    etag: str,
    config: Config,
):
    fasta_file = contig_fasta # config.run_id_dir / f"{runID}.fa"
    faa_file = config.run_id_dir / f"{runID}.faa"

    outfa = config.outdir / f"{key}.fa"
    outfaa =  config.outdir / f"{key}.faa"

    seq_record = SeqIO.read(fasta_file, "fasta")

    with open(outfa, "w") as output_handle1:

        sequence = seq_record.seq[start - 1 : end]

        ID = "_".join(seq_record.id.split("_")[-2:])

        seq_record.description = ""

        seq_record.seq = sequence
        seq_record.id = f"{id_dict[ID]} {start}-{end}"

        SeqIO.write(seq_record, output_handle1, "fasta")

    faa_records = SeqIO.parse(faa_file, "fasta")
    with open(outfaa, "w") as output_handle2:
        for faa_record in faa_records:
            seq_id = get_num(faa_record.id)
            if int(stag) <= seq_id <= int(etag):
                SeqIO.write(faa_record, output_handle2, "fasta")


def _meta(runID, input_fasta, config: Config):
    """Run ICEFinder2 in metagenomics mode"""

    contig_ids_mapping, renamed_fasta = rename_fasta(runID, input_fasta, config)

    prescanned_fasta_records = prescan(runID, renamed_fasta, config)

    fasta_stats_json = get_fasta_stats(runID, renamed_fasta, config)

    # TODO: disabled kraken
    # drawout, sequence_taxonomy_mapping, report = get_kraken_taxonomy(runID)
    # copy_files(report, config.outdir)

    copy_files(fasta_stats_json, config.outdir)

    ICEsum = config.run_id_dir / (runID + "_ICEsum.json")
    ICEsumlist = []

    for idx, seq_record in enumerate(SeqIO.parse(renamed_fasta, "fasta")):
        if seq_record.id in prescanned_fasta_records:
            sequence_prefix = f"{runID}_{seq_record.id}"
            record_sequence = str(seq_record.seq)

            sequence_prefix_folder = config.tmp_dir / sequence_prefix
            sequence_prefix_folder.mkdir(exist_ok=True)

            # Contig fasta
            contig_fasta = sequence_prefix_folder / (sequence_prefix + ".fa")

            # TODO: We should use biopython here to keep it consistent
            with open(contig_fasta, "w") as contig_fasta_handler:
                contig_fasta_handler.write(
                    ">%s\n%s\n" % (sequence_prefix, record_sequence)
                )

            # Annotate with prokka
            # TODO: If I'm reading this correctly, it's running prokka again (even though it ran prokka on the whole set of contigs)

            # The output files .faa and .gbk are stored directly in this process
            annotation_faa, annotation_fa, annotation_gff, annotation_ffn  = run_prokka(sequence_prefix, contig_fasta, config)

            ICEss = get_map(
                sequence_prefix, contig_fasta, contig_ids_mapping, annotation_faa, annotation_fa, annotation_gff, annotation_ffn, config
            )

            if ICEss:
                for key, value in ICEss.items():
                    [start_str, end_str, start_tag, end_tag] = value.split("|")
                    start = int(start_str)
                    end = int(end_str)
                    length = start - end + 1
                    ICEs = {
                        "id": str(idx),
                        "seqid": contig_ids_mapping[seq_record.id],
                        # "species": sequence_taxonomy_mapping[seq_record.id], TODO: disabled kraken2
                        "location": f"{start}..{end}",
                        "length": length,
                        "detail": key,
                    }

                    ICEsumlist.append(ICEs)

                    get_fasta(
                        runID,
                        contig_fasta,
                        contig_ids_mapping,
                        key,
                        start,
                        end,
                        start_tag,
                        end_tag,
                        config,
                    )

    with open(ICEsum, "w") as ice_file:
        json.dump(ICEsumlist, ice_file, indent=4)

    # Copy the js accesory files
    copy_files(config.icefinder2_static_directory, str(config.outdir / "js"))

    # TODO: do we need this?
    # delete_folders_starting_with_keyword(tmp_dir, runID)
